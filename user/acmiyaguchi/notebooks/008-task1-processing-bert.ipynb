{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/31 14:56:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/31 14:56:02 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk-dev-20240331-144314.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f19d6f7edd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   DOCNO|                POST|                 PRE|                TEXT|_corrupt_record|filename|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   0_0_6|I'm trying to wor...|\\n\\nAlthough most...|I will not tell m...|           NULL|s_0.trec|\n",
      "|456_1_12|You're not like '...|In general though...|Oh, and if you're...|           NULL|s_1.trec|\n",
      "| 764_1_5|Maybe it's one of...|My past experienc...|But I still want ...|           NULL|s_1.trec|\n",
      "|651_0_28|\\n\\nWe all run ba...|\\n\\nSo this woman...|I couldn't even i...|           NULL|s_1.trec|\n",
      "| 268_1_3| Both were great,...|\\n\\nI've only had...|One a couple year...|           NULL|s_1.trec|\n",
      "|364_0_12|I started opening...|Even though I too...|Which I can under...|           NULL|s_1.trec|\n",
      "|765_0_33|Nowhere on my inv...|I ask about the t...|Words can not exp...|           NULL|s_1.trec|\n",
      "|409_0_18|\\n\\nCOMMUNIST DAU...|The mention of a ...|The last stanza d...|           NULL|s_1.trec|\n",
      "| 546_1_1|~ Isabelle Stenge...|                    |On the contrary, ...|           NULL|s_1.trec|\n",
      "|582_0_11|Boys would shove ...|Everything went d...|They decided to t...|           NULL|s_1.trec|\n",
      "|785_0_15|He called things ...|\\n\\nWe were both ...|I ended up being ...|           NULL|s_1.trec|\n",
      "|657_0_30|\\n\\nTL;DR: I didn...|I'm not sure what...|I'm sorry this wa...|           NULL|s_1.trec|\n",
      "|253_1_31|\\n\\n**TL;DR: I wa...|Then I got nauseo...|\\n\\nAll in all, i...|           NULL|s_1.trec|\n",
      "| 661_0_4| I feel like utte...|\\n\\n\\nAbout a yea...|It turned out I w...|           NULL|s_1.trec|\n",
      "|  54_0_0|He wouldn't tell ...|                    |Over the past wee...|           NULL|s_1.trec|\n",
      "|670_0_27|She gets it's not...|\\n\\n\\nI think I e...|\\n\\n\\nSo in some ...|           NULL|s_1.trec|\n",
      "| 914_0_3|\\n\\n\\nHe's attemp...|He plans on visit...|He's studying to ...|           NULL|s_1.trec|\n",
      "|574_0_10|They can't send y...|\\n\\nYou are letti...|You're 20 and an ...|           NULL|s_1.trec|\n",
      "| 190_0_2|\\n\\nSo 7 or so mi...|\\n\\nA few nights ...|So i put her hand...|           NULL|s_1.trec|\n",
      "|678_0_29|All end on first ...|with**\\n\\n**EDIT*...|\\n\\n- Never had a...|           NULL|s_1.trec|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15542200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|   DOCNO|                TEXT|filename|\n",
      "+--------+--------------------+--------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4264693"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets also load up the train df\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "train_df.show()\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+\n",
      "|   DOCNO|                TEXT|filename|dataset|\n",
      "+--------+--------------------+--------+-------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|\n",
      "+--------+--------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 15:07:32 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ")\n",
    "\n",
    "subset_df = total_df.limit(100).cache()\n",
    "subset_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import (\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    Param,\n",
    "    Params,\n",
    "    TypeConverters,\n",
    ")\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "\n",
    "\"\"\"\n",
    "Wrapper for BERT to add it to the pipeline\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BertTransformer(\n",
    "    Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable\n",
    "):\n",
    "    input_col = Param(\n",
    "        Params._dummy(),\n",
    "        \"input_col\",\n",
    "        \"input column name.\",\n",
    "        typeConverter=TypeConverters.toString,\n",
    "    )\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\")\n",
    "\n",
    "    def make_predict_fn(self):\n",
    "        \"\"\"Return PredictBatchFunction\"\"\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "            return model.encode(inputs)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super().__init__()\n",
    "        self._setDefault(inputCol=input_col, outputCol=output_col)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "\n",
    "        # batch prediction UDF\n",
    "        apply_predict_batch = predict_batch_udf(\n",
    "            make_predict_fn=self.make_predict_fn,\n",
    "            return_type=ArrayType(FloatType()),\n",
    "            batch_size=8,\n",
    "        )\n",
    "\n",
    "        return df.withColumn(output_col, apply_predict_batch(input_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+--------------------+\n",
      "|   DOCNO|                TEXT|filename|dataset|          embeddings|\n",
      "+--------+--------------------+--------+-------+--------------------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train|[0.0180698, 0.004...|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|[-0.13028191, 0.0...|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|[-0.020124434, 0....|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|[-0.046469197, 0....|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|[-0.07952062, 0.0...|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|[0.0138995685, 0....|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|[-0.05409137, 0.0...|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|[-0.11549768, 0.1...|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|[-0.11340375, 0.1...|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|[-0.024256464, 0....|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|[-0.06978254, 0.0...|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|[0.043389082, 0.0...|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|[-0.022357611, 0....|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|[-0.098469354, 0....|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|[-0.08267389, 0.0...|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|[-0.049152568, 0....|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|[-0.1250817, 0.14...|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|[-0.055058602, 0....|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|[-0.046956237, 0....|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|[-0.013370703, 0....|\n",
      "+--------+--------------------+--------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 12.5 ms, sys: 124 µs, total: 12.6 ms\n",
      "Wall time: 4.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformer = BertTransformer(input_col=\"TEXT\", output_col=\"embeddings\")\n",
    "pipeline = Pipeline(stages=[transformer])\n",
    "\n",
    "pipeline_model = pipeline.fit(subset_df)\n",
    "%time pipeline_model.transform(subset_df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
