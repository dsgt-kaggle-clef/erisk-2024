{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's copy the training trec files locally\n",
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "dst = \"/mnt/data/task1/train\"\n",
    "! mkdir -p {dst}\n",
    "# copy files quietly\n",
    "! gcloud storage rsync -r \"{bucket}/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/\" {dst}/ &> /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/anthony/.ivy2/cache\n",
      "The jars for the packages stored in: /home/anthony/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-97597804-e448-4bef-af14-41e2c941b6a7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.17.0/spark-xml_2.12-0.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.17.0!spark-xml_2.12.jar (53ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (71ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (47ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.9.0/scala-collection-compat_2.12-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.9.0!scala-collection-compat_2.12.jar (57ms)\n",
      ":: resolution report :: resolve 4305ms :: artifacts dl 272ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-97597804-e448-4bef-af14-41e2c941b6a7\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (989kB/12ms)\n",
      "24/03/17 11:09:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 11:09:32 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x792266f82980>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "dst = \"/mnt/data/task1/train\"\n",
    "spark = get_spark(**{\"spark.jars.packages\": \"com.databricks:spark-xml_2.12:0.17.0\"})\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/task1/train:\n",
      "g_qrels_majority_2.csv\n",
      "g_rels_consenso.csv\n",
      "new_data\n",
      "\n",
      "/mnt/data/task1/train/new_data:\n",
      "s_0.trec\n",
      "s_1.trec\n",
      "s_10.trec\n",
      "s_100.trec\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! ls -R {dst} | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <DOC>\n",
      "            \t<DOCNO>s_0_0_0</DOCNO>\n",
      "            \t<TEXT>1.ye katiliyorum</TEXT>\n",
      "        </DOC>\n",
      "        <DOC>\n",
      "            \t<DOCNO>s_0_1_0</DOCNO>\n",
      "            \t<TEXT>ok haklsn abi gold atar msn</TEXT>\n",
      "        </DOC>\n",
      "        <DOC>\n",
      "            \t<DOCNO>s_0_2_0</DOCNO>\n"
     ]
    }
   ],
   "source": [
    "! cat {dst}/new_data/s_0.trec | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:(1715 + 4) / 3105][Stage 18:>(0 + 0) / 3105][Stage 19:>   (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 11:21:08 WARN BlockManager: Putting block rdd_107_2439 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/03/17 11:21:08 WARN BlockManager: Block rdd_107_2439 could not be removed as it was not found on disk or in memory\n",
      "24/03/17 11:21:08 WARN TaskSetManager: Lost task 2439.0 in stage 17.0 (TID 8664) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 17 cancelled )\n",
      "24/03/17 11:21:08 WARN BlockManager: Putting block rdd_107_2440 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/03/17 11:21:08 WARN BlockManager: Block rdd_107_2440 could not be removed as it was not found on disk or in memory\n",
      "24/03/17 11:21:08 WARN TaskSetManager: Lost task 2440.0 in stage 17.0 (TID 8665) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 17 cancelled )\n",
      "24/03/17 11:21:08 WARN BlockManager: Putting block rdd_107_2441 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/03/17 11:21:08 WARN BlockManager: Block rdd_107_2441 could not be removed as it was not found on disk or in memory\n",
      "24/03/17 11:21:08 WARN TaskSetManager: Lost task 2441.0 in stage 17.0 (TID 8666) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 17 cancelled )\n",
      "24/03/17 11:21:09 WARN BlockManager: Putting block rdd_107_2442 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/03/17 11:21:09 WARN BlockManager: Block rdd_107_2442 could not be removed as it was not found on disk or in memory\n",
      "24/03/17 11:21:09 WARN TaskSetManager: Lost task 2442.0 in stage 17.0 (TID 8667) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 17 cancelled )\n",
      "24/03/17 11:21:12 WARN TaskSetManager: Lost task 248.0 in stage 18.0 (TID 8916) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 18 cancelled )\n",
      "24/03/17 11:21:12 WARN TaskSetManager: Lost task 246.0 in stage 18.0 (TID 8914) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 18 cancelled )\n",
      "24/03/17 11:21:12 WARN TaskSetManager: Lost task 242.0 in stage 18.0 (TID 8910) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 18 cancelled )\n",
      "24/03/17 11:21:12 WARN TaskSetManager: Lost task 245.0 in stage 18.0 (TID 8913) (erisk-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job 18 cancelled )\n",
      "[Stage 21:===================================================>(3104 + 1) / 3105]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------------------------------+--------+\n",
      "|   DOCNO|                                                                            TEXT|filename|\n",
      "+--------+--------------------------------------------------------------------------------+--------+\n",
      "| s_0_0_0|                                                                1.ye katiliyorum|s_0.trec|\n",
      "| s_0_1_0|                                                     ok haklsn abi gold atar msn|s_0.trec|\n",
      "| s_0_2_0|                                                             almanca yarrak gibi|s_0.trec|\n",
      "| s_0_3_0|hani u oyunlarn en gl en gizemli silah, zellii falan olan eylere garip isimle...|s_0.trec|\n",
      "| s_0_3_1|dead cellste ygdar orus li ox var mesela, deus ex machina da olabilir onun gi...|s_0.trec|\n",
      "| s_0_3_2|                bunlarn bir dili falan var m yoksa kendileri mi uydurmu yapmclar|s_0.trec|\n",
      "| s_0_4_0|                                                         lnce diriltiyor sanirim|s_0.trec|\n",
      "| s_0_6_0|                                                                   ziya gzel sal|s_0.trec|\n",
      "| s_0_7_0|                                                              artk dedem deilsin|s_0.trec|\n",
      "| s_0_8_0|                                             sorma bizim matematiki de szelciymi|s_0.trec|\n",
      "| s_0_9_0|240 Volt FUCKMASTER Pro 5000 patlamayan, kauuk kaplama, 6 kat ayarlanabilir h...|s_0.trec|\n",
      "|s_0_10_0|                                                      bunlar nerden evirebilirim|s_0.trec|\n",
      "|s_0_11_0|                                    beynine gidecek kan sikine gidince byle olmu|s_0.trec|\n",
      "|s_0_12_0|                                        semeyen vizyonsuz dar grl orospu ocuudur|s_0.trec|\n",
      "|s_0_13_0|                                                                   ok haklsn abi|s_0.trec|\n",
      "|s_0_14_0|ilkokul zamanlarinda asiri usengec oldugum icin her isimi yatarak yapardim bi...|s_0.trec|\n",
      "|s_0_15_0|                                                      iliki kurmakta zorlanyorum|s_0.trec|\n",
      "|s_0_15_1|                                                        liseye giden bir gencim.|s_0.trec|\n",
      "|s_0_15_2|   tipimin ve kiiliimin ortalama olduunu dnyorum, fiziim ise kt denebilir zayfm.|s_0.trec|\n",
      "|s_0_15_3|                                                     ben insanlarla konuamyorum.|s_0.trec|\n",
      "+--------+--------------------------------------------------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read trec files into spark with text\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "\n",
    "# https://stackoverflow.com/questions/50429315/read-xml-in-spark\n",
    "df = (\n",
    "    spark.read.format(\"com.databricks.spark.xml\")\n",
    "    .option(\"rowTag\", \"DOC\")\n",
    "    .load(f\"{dst}/new_data/*.trec\")\n",
    "    # get the filename\n",
    "    .withColumn(\"filename\", F.udf(lambda p: Path(p).name)(F.input_file_name()))\n",
    ").cache()\n",
    "\n",
    "df.show(truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write this to gcs\n",
    "df.repartition(1).write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/train/\n",
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/train/_SUCCESS\n",
      "210.82MiB    gs://dsgt-clef-erisk-2024/task1/parquet/train/part-00000-2c958078-9a2d-4198-98f9-f6ec6156d701-c000.snappy.parquet\n",
      "210.82MiB    gs://dsgt-clef-erisk-2024/task1/parquet/train/\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage du --readable-sizes {bucket}/task1/parquet/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parquet\n",
    "\n",
    "We might as well do the test parquet while we're at it, since the process is exactly the same.\n",
    "Let's see if we can do it from gcs directly this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <DOC>\n",
      "            \t<DOCNO>0_0_0</DOCNO>\n",
      "            \t<PRE></PRE>\n",
      "            \t<TEXT>I guess it depends on what cheating entails.</TEXT>\n",
      "            \t<POST>I met someone who understands me better than I understand myself.</POST>\n",
      "        </DOC>\n",
      "        <DOC>\n",
      "            \t<DOCNO>0_0_1</DOCNO>\n",
      "            \t<PRE></PRE>\n",
      "            \t<TEXT>I met someone who understands me better than I understand myself.</TEXT>\n",
      "            \t<POST>Our friendship thus far is completely platonic, but I know if I had the opportunity, guilt-free I would pounce on dat ass.</POST>\n",
      "        </DOC>\n",
      "        <DOC>\n",
      "            \t<DOCNO>0_0_2</DOCNO>\n",
      "            \t<PRE>I met someone who understands me better than I understand myself.</PRE>\n",
      "            \t<TEXT>Our friendship thus far is completely platonic, but I know if I had the opportunity, guilt-free I would pounce on dat ass.</TEXT>\n",
      "            \t<POST>\n",
      "\n",
      "I tried to break up with my sweet SO, saying that I just hadn't been feeling the same way I had when we met (not a technical lie), but he begged to make it work and I figured I would give it a shot because, although the other guy is wide open, I know he still has feelings for his ex.</POST>\n",
      "        </DOC>\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage cat gs://dsgt-clef-erisk-2024/task1/dataset/s_0.trec | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================================>(552 + 1) / 553]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------+--------+\n",
      "| DOCNO|                                                                            POST|                                                                             PRE|                                                                            TEXT|_corrupt_record|filename|\n",
      "+------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------+--------+\n",
      "| 0_0_0|               I met someone who understands me better than I understand myself.|                                                                                |                                    I guess it depends on what cheating entails.|           NULL|s_0.trec|\n",
      "| 0_0_1|Our friendship thus far is completely platonic, but I know if I had the oppor...|                                                                                |               I met someone who understands me better than I understand myself.|           NULL|s_0.trec|\n",
      "| 0_0_2|\\n\\nI tried to break up with my sweet SO, saying that I just hadn't been feel...|               I met someone who understands me better than I understand myself.|Our friendship thus far is completely platonic, but I know if I had the oppor...|           NULL|s_0.trec|\n",
      "| 0_0_3|             It's been a year but it's noticeable he's still not stable from it.|Our friendship thus far is completely platonic, but I know if I had the oppor...|\\n\\nI tried to break up with my sweet SO, saying that I just hadn't been feel...|           NULL|s_0.trec|\n",
      "| 0_0_4|\\n\\nAlthough most wouldn't consider it cheating, I'm harboring feelings for a...|\\n\\nI tried to break up with my sweet SO, saying that I just hadn't been feel...|             It's been a year but it's noticeable he's still not stable from it.|           NULL|s_0.trec|\n",
      "| 0_0_5|                    I will not tell my SO because he would read further into it.|             It's been a year but it's noticeable he's still not stable from it.|\\n\\nAlthough most wouldn't consider it cheating, I'm harboring feelings for a...|           NULL|s_0.trec|\n",
      "| 0_0_6|               I'm trying to work on breaking it off, but my SO is too stubborn.|\\n\\nAlthough most wouldn't consider it cheating, I'm harboring feelings for a...|                    I will not tell my SO because he would read further into it.|           NULL|s_0.trec|\n",
      "| 0_0_7|                                                   \\n\\n**tl;dr** I'm confused. )|                    I will not tell my SO because he would read further into it.|               I'm trying to work on breaking it off, but my SO is too stubborn.|           NULL|s_0.trec|\n",
      "| 0_0_8|                                                                                |               I'm trying to work on breaking it off, but my SO is too stubborn.|                                                   \\n\\n**tl;dr** I'm confused. )|           NULL|s_0.trec|\n",
      "| 1_0_0|                                              \\n\\n\\nI am completely heartbroken.|                                                                                |                 Just found out my boyfriend of 3 years has been cheating on me.|           NULL|s_1.trec|\n",
      "| 1_0_1|                                                I don’t even know where to turn.|                                                                                |                                              \\n\\n\\nI am completely heartbroken.|           NULL|s_1.trec|\n",
      "| 1_0_2|Since we started dating I kind of gave up my life and moved to another city t...|                                              \\n\\n\\nI am completely heartbroken.|                                                I don’t even know where to turn.|           NULL|s_1.trec|\n",
      "| 1_0_3|     I can’t stop crying and i’ve just been playing back the history in my mind.|                                                I don’t even know where to turn.|Since we started dating I kind of gave up my life and moved to another city t...|           NULL|s_1.trec|\n",
      "| 1_0_4|                                       We talked about building a life together.|Since we started dating I kind of gave up my life and moved to another city t...|     I can’t stop crying and i’ve just been playing back the history in my mind.|           NULL|s_1.trec|\n",
      "| 1_0_5|I come from a broken family and I always valued his views on marriage and fid...|     I can’t stop crying and i’ve just been playing back the history in my mind.|                                       We talked about building a life together.|           NULL|s_1.trec|\n",
      "| 1_0_6|                                    We promised each other we would never cheat.|                                       We talked about building a life together.|I come from a broken family and I always valued his views on marriage and fid...|           NULL|s_1.trec|\n",
      "| 1_0_7|He reassured me over and over again that he only wanted to be with me and cou...|I come from a broken family and I always valued his views on marriage and fid...|                                    We promised each other we would never cheat.|           NULL|s_1.trec|\n",
      "| 1_0_8|                         I never imagined he could do something like this to me.|                                    We promised each other we would never cheat.|He reassured me over and over again that he only wanted to be with me and cou...|           NULL|s_1.trec|\n",
      "| 1_0_9|\\n\\n The crazy thing is that I accidentally saw a message from a girl on his ...|He reassured me over and over again that he only wanted to be with me and cou...|                         I never imagined he could do something like this to me.|           NULL|s_1.trec|\n",
      "|1_0_10|                               How could I completely forget about it until now?|                         I never imagined he could do something like this to me.|\\n\\n The crazy thing is that I accidentally saw a message from a girl on his ...|           NULL|s_1.trec|\n",
      "+------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.format(\"com.databricks.spark.xml\")\n",
    "    .option(\"rowTag\", \"DOC\")\n",
    "    .load(f\"{bucket}/task1/dataset/*.trec\")\n",
    "    # get the filename\n",
    "    .withColumn(\"filename\", F.udf(lambda p: Path(p).name)(F.input_file_name()))\n",
    ").cache()\n",
    "\n",
    "df.show(truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(8).write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
