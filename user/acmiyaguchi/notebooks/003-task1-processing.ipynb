{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc3e02",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "We use serverless dataproc to handle processing here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b2d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1187150f-6c4f-4ca3-b759-e6d929f8d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 18:27:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/17 18:27:23 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x794b301928c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=8, memory=\"28g\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b089f63-408b-4700-a442-923036574f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   DOCNO|                POST|                 PRE|                TEXT|_corrupt_record|filename|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   0_0_6|I'm trying to wor...|\\n\\nAlthough most...|I will not tell m...|           NULL|s_0.trec|\n",
      "|456_1_12|You're not like '...|In general though...|Oh, and if you're...|           NULL|s_1.trec|\n",
      "| 764_1_5|Maybe it's one of...|My past experienc...|But I still want ...|           NULL|s_1.trec|\n",
      "|651_0_28|\\n\\nWe all run ba...|\\n\\nSo this woman...|I couldn't even i...|           NULL|s_1.trec|\n",
      "| 268_1_3| Both were great,...|\\n\\nI've only had...|One a couple year...|           NULL|s_1.trec|\n",
      "|364_0_12|I started opening...|Even though I too...|Which I can under...|           NULL|s_1.trec|\n",
      "|765_0_33|Nowhere on my inv...|I ask about the t...|Words can not exp...|           NULL|s_1.trec|\n",
      "|409_0_18|\\n\\nCOMMUNIST DAU...|The mention of a ...|The last stanza d...|           NULL|s_1.trec|\n",
      "| 546_1_1|~ Isabelle Stenge...|                    |On the contrary, ...|           NULL|s_1.trec|\n",
      "|582_0_11|Boys would shove ...|Everything went d...|They decided to t...|           NULL|s_1.trec|\n",
      "|785_0_15|He called things ...|\\n\\nWe were both ...|I ended up being ...|           NULL|s_1.trec|\n",
      "|657_0_30|\\n\\nTL;DR: I didn...|I'm not sure what...|I'm sorry this wa...|           NULL|s_1.trec|\n",
      "|253_1_31|\\n\\n**TL;DR: I wa...|Then I got nauseo...|\\n\\nAll in all, i...|           NULL|s_1.trec|\n",
      "| 661_0_4| I feel like utte...|\\n\\n\\nAbout a yea...|It turned out I w...|           NULL|s_1.trec|\n",
      "|  54_0_0|He wouldn't tell ...|                    |Over the past wee...|           NULL|s_1.trec|\n",
      "|670_0_27|She gets it's not...|\\n\\n\\nI think I e...|\\n\\n\\nSo in some ...|           NULL|s_1.trec|\n",
      "| 914_0_3|\\n\\n\\nHe's attemp...|He plans on visit...|He's studying to ...|           NULL|s_1.trec|\n",
      "|574_0_10|They can't send y...|\\n\\nYou are letti...|You're 20 and an ...|           NULL|s_1.trec|\n",
      "| 190_0_2|\\n\\nSo 7 or so mi...|\\n\\nA few nights ...|So i put her hand...|           NULL|s_1.trec|\n",
      "|678_0_29|All end on first ...|with**\\n\\n**EDIT*...|\\n\\n- Never had a...|           NULL|s_1.trec|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b51a56-03d0-4a42-acfd-fe2a9ea7e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+----+---------------+--------+\n",
      "|DOCNO|POST|PRE|TEXT|_corrupt_record|filename|\n",
      "+-----+----+---+----+---------------+--------+\n",
      "+-----+----+---+----+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.where(\"_corrupt_record <> null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8c90e0-58a1-4776-9c22-9f1131b17de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==========================================>              (18 + 6) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  filename|count|\n",
      "+----------+-----+\n",
      "|s_153.trec|27966|\n",
      "|s_196.trec|26988|\n",
      "|s_194.trec|28570|\n",
      "|s_265.trec|26173|\n",
      "|s_165.trec|28915|\n",
      "|s_272.trec|29164|\n",
      "| s_15.trec|29058|\n",
      "|s_220.trec|27630|\n",
      "|s_118.trec|27238|\n",
      "|s_133.trec|26989|\n",
      "|s_111.trec|26887|\n",
      "|s_157.trec|26094|\n",
      "|s_277.trec|26536|\n",
      "|s_180.trec|26234|\n",
      "|s_193.trec|28355|\n",
      "|s_115.trec|30587|\n",
      "|s_158.trec|27811|\n",
      "|s_268.trec|28074|\n",
      "|  s_1.trec|27087|\n",
      "|s_280.trec|27301|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.groupby(\"filename\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b135259-8258-4ab9-9f44-b4bbba6ebd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15542200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ba5d6e-a151-4e33-9b2a-22d04ab0c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|   DOCNO|                TEXT|filename|\n",
      "+--------+--------------------+--------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4264693"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets also load up the train df\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "train_df.show()\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8211c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "|   DOCNO|                TEXT|filename|               words|           hashingtf|               tfidf|\n",
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec| [1.ye, katiliyorum]|(256,[18,181],[1....|(256,[18,181],[3....|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|[ok, haklsn, abi,...|(256,[53,77,118,1...|(256,[53,77,118,1...|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|[almanca, yarrak,...|(256,[78,108,148]...|(256,[78,108,148]...|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|[hani, u, oyunlar...|(256,[41,47,50,71...|(256,[41,47,50,71...|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|[dead, cellste, y...|(256,[2,11,47,53,...|(256,[2,11,47,53,...|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|[bunlarn, bir, di...|(256,[8,26,44,83,...|(256,[8,26,44,83,...|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|[lnce, diriltiyor...|(256,[92,127,233]...|(256,[92,127,233]...|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|   [ziya, gzel, sal]|(256,[113,211,222...|(256,[113,211,222...|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|[artk, dedem, dei...|(256,[29,71,211],...|(256,[29,71,211],...|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|[sorma, bizim, ma...|(256,[1,116,148,1...|(256,[1,116,148,1...|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|[240, volt, fuckm...|(256,[2,3,19,21,2...|(256,[2,3,19,21,2...|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|[bunlar, nerden, ...|(256,[115,117,142...|(256,[115,117,142...|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|[beynine, gidecek...|(256,[10,72,118,1...|(256,[10,72,118,1...|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|[semeyen, vizyons...|(256,[28,91,122,1...|(256,[28,91,122,1...|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|   [ok, haklsn, abi]|(256,[53,77,131],...|(256,[53,77,131],...|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|[ilkokul, zamanla...|(256,[13,21,33,50...|(256,[13,21,33,50...|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|[iliki, kurmakta,...|(256,[27,129,217]...|(256,[27,129,217]...|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|[liseye, giden, b...|(256,[67,161,186,...|(256,[67,161,186,...|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|[tipimin, ve, kii...|(256,[21,66,87,11...|(256,[21,66,87,11...|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|[ben, insanlarla,...|(256,[17,57,230],...|(256,[17,57,230],...|\n",
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# let's apply tf-idf to the text column, and also include word2vec\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashing_features = 256\n",
    "word2vec_features = 256\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol=\"hashingtf\",\n",
    "    numFeatures=hashing_features,\n",
    ")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"tfidf\")\n",
    "# word2vec = Word2Vec(\n",
    "#     vectorSize=word2vec_features,\n",
    "#     minCount=0,\n",
    "#     inputCol=tokenizer.getOutputCol(),\n",
    "#     outputCol=\"word2vec\",\n",
    "# )\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pipeline_model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "923e42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+\n",
      "|   DOCNO|                TEXT|filename|dataset|\n",
      "+--------+--------------------+--------+-------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|\n",
      "+--------+--------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19806893"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a new dataframe with features from both train and test\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ")\n",
    "total_df.show()\n",
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a0f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hashingtf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- tfidf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "pipeline_model = pipeline.fit(total_df)\n",
    "pipeline_model.write().overwrite().save(f\"{bucket}/task1/models/pipeline_tfidf\")\n",
    "print(\"pipeline model saved\")\n",
    "\n",
    "res_df = (\n",
    "    pipeline_model.transform(total_df)\n",
    "    .withColumn(\"hashingtf\", vector_to_array(F.col(\"hashingtf\")))\n",
    "    .withColumn(\"tfidf\", vector_to_array(F.col(\"tfidf\")))\n",
    ")\n",
    "res_df.printSchema()\n",
    "\n",
    "# save both the pipeline and the total_df\n",
    "res_df.write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/combined_tfidf\")\n",
    "print(\"combined tfidf saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61471c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n",
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/_SUCCESS\n",
      "1.04kiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00000-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "472.95MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00002-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "299.32MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00006-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "409.05MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00008-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.82MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00009-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "408.61MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00010-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.09MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00011-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.58MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00012-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "410.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00013-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.79MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00014-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.57MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00015-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.09MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00016-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.49MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00017-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00018-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.24MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00019-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "409.06MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00020-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "408.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00021-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.89MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00022-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.38MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00023-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.05MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00024-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "310.80MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00025-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.33MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00026-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.10MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00027-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.26MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00028-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.52MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00029-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "311.96MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00030-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.89MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00031-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "9.56GiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage du --readable-sizes {bucket}/task1/parquet/combined_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb524c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "|   DOCNO|                TEXT|filename|dataset|               words|           hashingtf|               tfidf|\n",
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train| [1.ye, katiliyorum]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|[ok, haklsn, abi,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|[almanca, yarrak,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|[hani, u, oyunlar...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|[dead, cellste, y...|[0.0, 0.0, 1.0, 0...|[0.0, 0.0, 1.1335...|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|[bunlarn, bir, di...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|[lnce, diriltiyor...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|   [ziya, gzel, sal]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|[artk, dedem, dei...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|[sorma, bizim, ma...|[0.0, 1.0, 0.0, 0...|[0.0, 2.912064239...|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|[240, volt, fuckm...|[0.0, 0.0, 1.0, 2...|[0.0, 0.0, 1.1335...|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|[bunlar, nerden, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|[beynine, gidecek...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|[semeyen, vizyons...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|   [ok, haklsn, abi]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|[ilkokul, zamanla...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|[iliki, kurmakta,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|[liseye, giden, b...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|[tipimin, ve, kii...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|[ben, insanlarla,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_df = spark.read.parquet(f\"{bucket}/task1/parquet/combined_tfidf\")\n",
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c4d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
