{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc3e02",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "We use serverless dataproc to handle processing here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1187150f-6c4f-4ca3-b759-e6d929f8d075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gdpic-srvls-session-97861cc8-eebc-45d7-844f-8742966256a8-m.us-central1-b.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://gdpic-srvls-session-97861cc8-eebc-45d7-844f-8742966256a8-m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f14ca855e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b089f63-408b-4700-a442-923036574f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   DOCNO|                POST|                 PRE|                TEXT|_corrupt_record|filename|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   0_0_6|I'm trying to wor...|\\n\\nAlthough most...|I will not tell m...|           null|s_0.trec|\n",
      "|456_1_12|You're not like '...|In general though...|Oh, and if you're...|           null|s_1.trec|\n",
      "| 764_1_5|Maybe it's one of...|My past experienc...|But I still want ...|           null|s_1.trec|\n",
      "|651_0_28|\\n\\nWe all run ba...|\\n\\nSo this woman...|I couldn't even i...|           null|s_1.trec|\n",
      "| 268_1_3| Both were great,...|\\n\\nI've only had...|One a couple year...|           null|s_1.trec|\n",
      "|364_0_12|I started opening...|Even though I too...|Which I can under...|           null|s_1.trec|\n",
      "|765_0_33|Nowhere on my inv...|I ask about the t...|Words can not exp...|           null|s_1.trec|\n",
      "|409_0_18|\\n\\nCOMMUNIST DAU...|The mention of a ...|The last stanza d...|           null|s_1.trec|\n",
      "| 546_1_1|~ Isabelle Stenge...|                    |On the contrary, ...|           null|s_1.trec|\n",
      "|582_0_11|Boys would shove ...|Everything went d...|They decided to t...|           null|s_1.trec|\n",
      "|785_0_15|He called things ...|\\n\\nWe were both ...|I ended up being ...|           null|s_1.trec|\n",
      "|657_0_30|\\n\\nTL;DR: I didn...|I'm not sure what...|I'm sorry this wa...|           null|s_1.trec|\n",
      "|253_1_31|\\n\\n**TL;DR: I wa...|Then I got nauseo...|\\n\\nAll in all, i...|           null|s_1.trec|\n",
      "| 661_0_4| I feel like utte...|\\n\\n\\nAbout a yea...|It turned out I w...|           null|s_1.trec|\n",
      "|  54_0_0|He wouldn't tell ...|                    |Over the past wee...|           null|s_1.trec|\n",
      "|670_0_27|She gets it's not...|\\n\\n\\nI think I e...|\\n\\n\\nSo in some ...|           null|s_1.trec|\n",
      "| 914_0_3|\\n\\n\\nHe's attemp...|He plans on visit...|He's studying to ...|           null|s_1.trec|\n",
      "|574_0_10|They can't send y...|\\n\\nYou are letti...|You're 20 and an ...|           null|s_1.trec|\n",
      "| 190_0_2|\\n\\nSo 7 or so mi...|\\n\\nA few nights ...|So i put her hand...|           null|s_1.trec|\n",
      "|678_0_29|All end on first ...|with**\\n\\n**EDIT*...|\\n\\n- Never had a...|           null|s_1.trec|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b51a56-03d0-4a42-acfd-fe2a9ea7e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+----+---------------+--------+\n",
      "|DOCNO|POST|PRE|TEXT|_corrupt_record|filename|\n",
      "+-----+----+---+----+---------------+--------+\n",
      "+-----+----+---+----+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.where(\"_corrupt_record <> null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8c90e0-58a1-4776-9c22-9f1131b17de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  filename|count|\n",
      "+----------+-----+\n",
      "|s_153.trec|27966|\n",
      "|s_196.trec|26988|\n",
      "|s_194.trec|28570|\n",
      "|s_265.trec|26173|\n",
      "|s_165.trec|28915|\n",
      "|s_272.trec|29164|\n",
      "| s_15.trec|29058|\n",
      "|s_220.trec|27630|\n",
      "|s_118.trec|27238|\n",
      "|s_133.trec|26989|\n",
      "|s_111.trec|26887|\n",
      "|s_157.trec|26094|\n",
      "|s_277.trec|26536|\n",
      "|s_180.trec|26234|\n",
      "|s_193.trec|28355|\n",
      "|s_115.trec|30587|\n",
      "|s_158.trec|27811|\n",
      "|s_268.trec|28074|\n",
      "|  s_1.trec|27087|\n",
      "|s_280.trec|27301|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.groupby(\"filename\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b135259-8258-4ab9-9f44-b4bbba6ebd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15542200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ba5d6e-a151-4e33-9b2a-22d04ab0c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|   DOCNO|                TEXT|filename|\n",
      "+--------+--------------------+--------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4264693"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets also load up the train df\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "train_df.show()\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8211c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "|   DOCNO|                TEXT|filename|               words|           hashingtf|               tfidf|\n",
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec| [1.ye, katiliyorum]|(256,[18,181],[1....|(256,[18,181],[3....|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|[ok, haklsn, abi,...|(256,[53,77,118,1...|(256,[53,77,118,1...|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|[almanca, yarrak,...|(256,[78,108,148]...|(256,[78,108,148]...|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|[hani, u, oyunlar...|(256,[41,47,50,71...|(256,[41,47,50,71...|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|[dead, cellste, y...|(256,[2,11,47,53,...|(256,[2,11,47,53,...|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|[bunlarn, bir, di...|(256,[8,26,44,83,...|(256,[8,26,44,83,...|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|[lnce, diriltiyor...|(256,[92,127,233]...|(256,[92,127,233]...|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|   [ziya, gzel, sal]|(256,[113,211,222...|(256,[113,211,222...|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|[artk, dedem, dei...|(256,[29,71,211],...|(256,[29,71,211],...|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|[sorma, bizim, ma...|(256,[1,116,148,1...|(256,[1,116,148,1...|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|[240, volt, fuckm...|(256,[2,3,19,21,2...|(256,[2,3,19,21,2...|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|[bunlar, nerden, ...|(256,[115,117,142...|(256,[115,117,142...|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|[beynine, gidecek...|(256,[10,72,118,1...|(256,[10,72,118,1...|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|[semeyen, vizyons...|(256,[28,91,122,1...|(256,[28,91,122,1...|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|   [ok, haklsn, abi]|(256,[53,77,131],...|(256,[53,77,131],...|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|[ilkokul, zamanla...|(256,[13,21,33,50...|(256,[13,21,33,50...|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|[iliki, kurmakta,...|(256,[27,129,217]...|(256,[27,129,217]...|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|[liseye, giden, b...|(256,[67,161,186,...|(256,[67,161,186,...|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|[tipimin, ve, kii...|(256,[21,66,87,11...|(256,[21,66,87,11...|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|[ben, insanlarla,...|(256,[17,57,230],...|(256,[17,57,230],...|\n",
      "+--------+--------------------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# let's apply tf-idf to the text column, and also include word2vec\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashing_features = 256\n",
    "word2vec_features = 256\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol=\"hashingtf\",\n",
    "    numFeatures=hashing_features,\n",
    ")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"tfidf\")\n",
    "# word2vec = Word2Vec(\n",
    "#     vectorSize=word2vec_features,\n",
    "#     minCount=0,\n",
    "#     inputCol=tokenizer.getOutputCol(),\n",
    "#     outputCol=\"word2vec\",\n",
    "# )\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pipeline_model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "923e42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+\n",
      "|   DOCNO|                TEXT|filename|dataset|\n",
      "+--------+--------------------+--------+-------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|\n",
      "+--------+--------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19806893"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a new dataframe with features from both train and test\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ")\n",
    "total_df.show()\n",
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a0f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hashingtf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- tfidf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "pipeline_model = pipeline.fit(total_df)\n",
    "res_df = (\n",
    "    pipeline_model.transform(total_df)\n",
    "    .withColumn(\"hashingtf\", vector_to_array(F.col(\"hashingtf\")))\n",
    "    .withColumn(\"tfidf\", vector_to_array(F.col(\"tfidf\")))\n",
    ")\n",
    "res_df.printSchema()\n",
    "\n",
    "# save both the pipeline and the total_df\n",
    "pipeline_model.write().overwrite().save(f\"{bucket}/task1/models/pipeline_tfidf\")\n",
    "total_df.write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/combined_tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61471c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n",
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/_SUCCESS\n",
      "2.14kiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00000-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "392.45MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00002-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "251.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00006-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.46MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00008-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.25MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00009-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.30MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00010-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.48MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00011-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.63MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00012-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "369.05MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00013-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.48MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00014-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.15MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00015-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.52MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00016-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.44MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00017-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.69MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00018-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.37MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00019-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.45MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00020-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.34MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00021-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "368.57MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00022-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "367.29MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00023-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "282.17MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00024-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "280.07MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00025-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "281.93MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00026-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "281.45MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00027-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "281.45MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00028-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "281.50MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00029-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "281.12MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00030-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "280.80MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00031-787e6764-55b3-4a6b-863f-4e8445368c5d-c000.snappy.parquet\n",
      "8.58GiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage du --readable-sizes {bucket}/task1/parquet/combined_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb524c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:09:39 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 238) (10.128.15.220 executor 1): java.lang.IllegalArgumentException: requirement failed: Index 0 follows 0 and is not strictly increasing\n",
      "\tat scala.Predef$.require(Predef.scala:337)\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.$anonfun$new$5(Vectors.scala:629)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1329)\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:628)\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/03/17 16:09:47 ERROR TaskSetManager: Task 0 in stage 44.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o580.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 241) (10.128.15.221 executor 0): java.lang.IllegalArgumentException: requirement failed: Index 0 follows 0 and is not strictly increasing\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.linalg.SparseVector.$anonfun$new$5(Vectors.scala:629)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1329)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:628)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2277)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2317)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:528)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Index 0 follows 0 and is not strictly increasing\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.linalg.SparseVector.$anonfun$new$5(Vectors.scala:629)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1329)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:628)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m total_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/task1/parquet/combined_tfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtotal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o580.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 241) (10.128.15.221 executor 0): java.lang.IllegalArgumentException: requirement failed: Index 0 follows 0 and is not strictly increasing\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.linalg.SparseVector.$anonfun$new$5(Vectors.scala:629)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1329)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:628)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2277)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2317)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:528)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Index 0 follows 0 and is not strictly increasing\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.linalg.SparseVector.$anonfun$new$5(Vectors.scala:629)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1329)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:628)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "total_df = spark.read.parquet(f\"{bucket}/task1/parquet/combined_tfidf\")\n",
    "total_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erisk-dev on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-00006e209354"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
