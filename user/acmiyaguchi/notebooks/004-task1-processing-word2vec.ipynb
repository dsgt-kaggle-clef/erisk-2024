{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc3e02",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "Let's add in word2vec to our processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b2d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1187150f-6c4f-4ca3-b759-e6d929f8d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 19:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/17 19:06:13 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c8d96f96560>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=8, memory=\"28g\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8211c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+--------------------+--------------------+\n",
      "|        DOCNO|                TEXT|   filename|               words|            word2vec|\n",
      "+-------------+--------------------+-----------+--------------------+--------------------+\n",
      "| s_1773_565_0| Yes the pot has ...|s_1773.trec|[, yes, the, pot,...|[-0.1233540048392...|\n",
      "|s_1418_1347_1|Is the show faith...|s_1418.trec|[is, the, show, f...|[-0.1363150575530...|\n",
      "| s_1447_631_0|The feeling is al...|s_1447.trec|[the, feeling, is...|[0.04741903394460...|\n",
      "|s_1452_832_14|I didn't know abo...|s_1452.trec|[i, didn't, know,...|[0.08563407436013...|\n",
      "|s_2302_231_12|This implies that...|s_2302.trec|[this, implies, t...|[-0.0261339815167...|\n",
      "|  s_195_798_1|Too bad there isn...| s_195.trec|[too, bad, there,...|[0.00197446951642...|\n",
      "| s_1419_351_0|Get $5 when you s...|s_1419.trec|[get, $5, when, y...|[-0.1175157446414...|\n",
      "|s_1452_1763_2|I wish he was com...|s_1452.trec|[i, wish, he, was...|[-0.0113339025499...|\n",
      "|  s_2347_98_5| I actually like ...|s_2347.trec|[, i, actually, l...|[-0.0134012909606...|\n",
      "| s_2088_672_1|When mine switche...|s_2088.trec|[when, mine, swit...|[-0.1271591790136...|\n",
      "|s_2613_1435_0|  Blursed_Nice beach|s_2613.trec|[blursed_nice, be...|[0.10879205632954...|\n",
      "| s_1881_783_1|I do mostly small...|s_1881.trec|[i, do, mostly, s...|[-0.0550950082580...|\n",
      "| s_1749_572_3|I don't really ha...|s_1749.trec|[i, don't, really...|[0.04873697020507...|\n",
      "| s_2025_220_3|He doesn't mentio...|s_2025.trec|[he, doesn't, men...|[-0.0450319893475...|\n",
      "| s_1040_880_5|\\n\\nWe have polit...|s_1040.trec|[, , we, have, po...|[-0.1070530017411...|\n",
      "|s_1601_1027_3|Maybe humour was ...|s_1601.trec|[maybe, humour, w...|[-0.0507965210606...|\n",
      "|s_1674_10_259|\\n\\nDannys worst ...|s_1674.trec|[, , dannys, wors...|[-0.1963786163500...|\n",
      "|s_100_1172_13|\\n\\nThanks for co...| s_100.trec|[, , thanks, for,...|[0.05524594419532...|\n",
      "| s_1255_293_7|I can't tell you ...|s_1255.trec|[i, can't, tell, ...|[-0.0154960324595...|\n",
      "| s_1983_901_0|People like you a...|s_1983.trec|[people, like, yo...|[-0.0164790538450...|\n",
      "+-------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.32 ms, sys: 1.05 ms, total: 10.4 ms\n",
      "Wall time: 8.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# let's apply tf-idf to the text column, and also include word2vec\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "\n",
    "word2vec_features = 64\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT\", outputCol=\"words\")\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=word2vec_features,\n",
    "    numPartitions=8,\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol=\"word2vec\",\n",
    ")\n",
    "pipeline = Pipeline(stages=[tokenizer, word2vec])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "%time pipeline_model.transform(train_df.repartition(8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a new dataframe with features from both train and test\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ").repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a0f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 20:56:41 WARN TaskSetManager: Stage 42 contains a task of very large size (41237 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline model saved\n",
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- word2vec: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined word2vec saved\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "pipeline_model = pipeline.fit(total_df)\n",
    "pipeline_model.write().overwrite().save(f\"{bucket}/task1/models/pipeline_word2vec\")\n",
    "print(\"pipeline model saved\")\n",
    "\n",
    "res_df = pipeline_model.transform(total_df).withColumn(\n",
    "    \"word2vec\", vector_to_array(F.col(\"word2vec\"))\n",
    ")\n",
    "res_df.printSchema()\n",
    "\n",
    "# save both the pipeline and the total_df\n",
    "res_df.write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/combined_word2vec\")\n",
    "print(\"combined word2vec saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61471c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/\n",
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/_SUCCESS\n",
      "993.76MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00000-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.65MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00001-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.82MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00002-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.31MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00003-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.74MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00004-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.86MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00005-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.92MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00006-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.54MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00007-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.55MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00008-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.81MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00009-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.32MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00010-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.88MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00011-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.83MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00012-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.78MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00013-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.69MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00014-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "993.15MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/part-00015-9b14448d-8d9c-4882-a2a1-4bcdbd4d01de-c000.snappy.parquet\n",
      "15.53GiB     gs://dsgt-clef-erisk-2024/task1/parquet/combined_word2vec/\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage du --readable-sizes {bucket}/task1/parquet/combined_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb524c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "|        DOCNO|                TEXT|   filename|dataset|               words|            word2vec|\n",
      "+-------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "| s_1065_569_8|I did not die or ...|s_1065.trec|  train|[i, did, not, die...|[-0.0302185551686...|\n",
      "|  s_1483_86_1|My wife left me 5...|s_1483.trec|  train|[my, wife, left, ...|[-0.0837481777582...|\n",
      "| s_191_268_30|Either discovery ...| s_191.trec|  train|[either, discover...|[0.03546374198049...|\n",
      "|  s_1785_23_1|Sorry I phrased i...|s_1785.trec|  train|[sorry, i, phrase...|[-0.1627588227391...|\n",
      "| s_2587_200_0| You'll always be...|s_2587.trec|  train|[, you'll, always...|[-0.0674093064541...|\n",
      "|s_2242_640_14|\\n\\nMy point in s...|s_2242.trec|  train|[, , my, point, i...|[-0.0760117996911...|\n",
      "| s_2731_623_3|The robot, of cou...|s_2731.trec|  train|[the, robot,, of,...|[-0.1646860837936...|\n",
      "|s_2544_1055_0|         What shader|s_2544.trec|  train|      [what, shader]|[0.00345687568187...|\n",
      "|s_1754_1481_1| Been trying to r...|s_1754.trec|  train|[, been, trying, ...|[0.08285433825637...|\n",
      "|s_2494_702_22|Please click and ...|s_2494.trec|  train|[please, click, a...|[0.17857133969664...|\n",
      "|   s_139_72_0|         mmmm, steak| s_139.trec|  train|    [, mmmm,, steak]|[0.23622255275646...|\n",
      "|   s_2379_1_9|Just don't know i...|s_2379.trec|  train|[just, don't, kno...|[-0.0208685226738...|\n",
      "| s_2362_0_302|\\tThe Beloved (Di...|s_2362.trec|  train|[, the, beloved, ...|[0.19053154755383...|\n",
      "|s_1900_1053_0| Zero - Because w...|s_1900.trec|  train|[, zero, -, becau...|[0.07111104331644...|\n",
      "| s_1987_17_17|This should be po...|s_1987.trec|  train|[this, should, be...|[0.06404028572142...|\n",
      "|s_1984_1583_1|Our goal is to cr...|s_1984.trec|  train|[our, goal, is, t...|[0.08148161570231...|\n",
      "| s_1305_115_1|I remember as a k...|s_1305.trec|  train|[i, remember, as,...|[-0.0305646524034...|\n",
      "| s_188_1122_8|\\n\\timport javafx...| s_188.trec|  train|[, , import, java...|[0.23011032119393...|\n",
      "| s_2248_933_1|Which, I think is...|s_2248.trec|  train|[which,, i, think...|[-0.0260325750069...|\n",
      "| s_1077_139_1|   The classic tool?|s_1077.trec|  train|[the, classic, to...|[-0.0167552133401...|\n",
      "+-------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_df = spark.read.parquet(f\"{bucket}/task1/parquet/combined_word2vec\")\n",
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39c75765",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
