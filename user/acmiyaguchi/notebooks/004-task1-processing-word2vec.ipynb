{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc3e02",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "Let's add in word2vec to our processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b2d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1187150f-6c4f-4ca3-b759-e6d929f8d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 19:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/17 19:06:13 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c8d96f96560>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=8, memory=\"28g\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8211c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# let's apply tf-idf to the text column, and also include word2vec\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "\n",
    "word2vec_features = 256\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT\", outputCol=\"words\")\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=word2vec_features,\n",
    "    minCount=0,\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol=\"word2vec\",\n",
    ")\n",
    "pipeline = Pipeline(stages=[tokenizer, word2vec])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "%time pipeline_model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+\n",
      "|   DOCNO|                TEXT|filename|dataset|\n",
      "+--------+--------------------+--------+-------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|\n",
      "+--------+--------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19806893"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a new dataframe with features from both train and test\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hashingtf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- tfidf: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "pipeline_model = pipeline.fit(total_df)\n",
    "pipeline_model.write().overwrite().save(f\"{bucket}/task1/models/pipeline_word2vec\")\n",
    "print(\"pipeline model saved\")\n",
    "\n",
    "res_df = pipeline_model.transform(total_df).withColumn(\n",
    "    \"word2vec\", vector_to_array(F.col(\"word2vec\"))\n",
    ")\n",
    "res_df.printSchema()\n",
    "\n",
    "# save both the pipeline and the total_df\n",
    "res_df.write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/combined_word2vec\")\n",
    "print(\"combined word2vec saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61471c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n",
      "0B           gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/_SUCCESS\n",
      "1.04kiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00000-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "472.95MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00002-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "299.32MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00006-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "409.05MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00008-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.82MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00009-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "408.61MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00010-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.09MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00011-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.58MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00012-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "410.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00013-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.79MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00014-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.57MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00015-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.09MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00016-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "405.49MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00017-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00018-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.24MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00019-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "409.06MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00020-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "408.01MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00021-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "406.89MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00022-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "407.38MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00023-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.05MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00024-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "310.80MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00025-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.33MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00026-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.10MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00027-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.26MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00028-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "312.52MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00029-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "311.96MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00030-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "313.89MiB    gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/part-00031-4ee3bfbe-fc38-48f5-a93a-1d50237f7fef-c000.snappy.parquet\n",
      "9.56GiB      gs://dsgt-clef-erisk-2024/task1/parquet/combined_tfidf/\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage du --readable-sizes {bucket}/task1/parquet/combined_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb524c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "|   DOCNO|                TEXT|filename|dataset|               words|           hashingtf|               tfidf|\n",
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|  train| [1.ye, katiliyorum]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|  train|[ok, haklsn, abi,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|  train|[almanca, yarrak,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|  train|[hani, u, oyunlar...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|  train|[dead, cellste, y...|[0.0, 0.0, 1.0, 0...|[0.0, 0.0, 1.1335...|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|  train|[bunlarn, bir, di...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|  train|[lnce, diriltiyor...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|  train|   [ziya, gzel, sal]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|  train|[artk, dedem, dei...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|  train|[sorma, bizim, ma...|[0.0, 1.0, 0.0, 0...|[0.0, 2.912064239...|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|  train|[240, volt, fuckm...|[0.0, 0.0, 1.0, 2...|[0.0, 0.0, 1.1335...|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|  train|[bunlar, nerden, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|  train|[beynine, gidecek...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|  train|[semeyen, vizyons...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|  train|   [ok, haklsn, abi]|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|  train|[ilkokul, zamanla...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|  train|[iliki, kurmakta,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|  train|[liseye, giden, b...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|  train|[tipimin, ve, kii...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|  train|[ben, insanlarla,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "+--------+--------------------+--------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_df = spark.read.parquet(f\"{bucket}/task1/parquet/combined_word2vec\")\n",
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c75765",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
