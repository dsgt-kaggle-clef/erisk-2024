{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At gs://dsgt-clef-erisk-2024/task1/processed/data/count/v3/data/**, worker process 377247 thread 139662684215104 listed 501...\n"
     ]
    }
   ],
   "source": [
    "# rsync the data locally\n",
    "! gcloud storage rsync -r \\\n",
    "    gs://dsgt-clef-erisk-2024/task1/processed/data/count/v3/data \\\n",
    "    /mnt/data/erisk/task1/processed/data/count/v3/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At gs://dsgt-clef-erisk-2024/task1/processed/data/word2vec/v3/data/**, worker process 374185 thread 140082212865856 listed 501...\n",
      "At file:///mnt/data/erisk/task1/processed/data/word2vec/v3/data/**, worker process 374185 thread 140082212865856 listed 501...\n",
      "  Completed files 0 | 0B                                                       \n"
     ]
    }
   ],
   "source": [
    "! gcloud storage rsync -r \\\n",
    "    gs://dsgt-clef-erisk-2024/task1/processed/data/word2vec/v3/data \\\n",
    "    /mnt/data/erisk/task1/processed/data/word2vec/v3/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At gs://dsgt-clef-erisk-2024/task1/processed/eval/logistic_word2vec/**, worker process 375339 thread 139913874536256 listed 503...\n",
      "At file:///mnt/data/erisk/task1/processed/eval/logistic_word2vec/**, worker process 375339 thread 139913874536256 listed 503...\n",
      "  Completed files 0 | 0B                                                       \n"
     ]
    }
   ],
   "source": [
    "! gcloud storage rsync -r \\\n",
    "    gs://dsgt-clef-erisk-2024/task1/processed/eval/logistic_word2vec \\\n",
    "    /mnt/data/erisk/task1/processed/eval/logistic_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://dsgt-clef-erisk-2024/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_rels_consenso.csv to file:///mnt/data/erisk/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_rels_consenso.csv\n",
      "  Completed files 1/1 | 406.8kiB/406.8kiB                                      \n"
     ]
    }
   ],
   "source": [
    "! gcloud storage cp \\\n",
    "    'gs://dsgt-clef-erisk-2024/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_rels_consenso.csv' \\\n",
    "    '/mnt/data/erisk/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_rels_consenso.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_prefix = \"gs://dsgt-clef-erisk-2024\"\n",
    "local_prefix = \"/mnt/data/erisk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- docid: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- word2vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "+------------+--------------------+-----------+-------+--------------------+--------------------+--------------------+\n",
      "|       docid|                text|   filename|dataset|               words|      filtered_words|            word2vec|\n",
      "+------------+--------------------+-----------+-------+--------------------+--------------------+--------------------+\n",
      "| 321176_0_16|Surprisingly, it ...| s_322.trec|   test|[surprisingly,, i...|[surprisingly,, w...|[0.00493731946750...|\n",
      "|  414896_0_3|Things were good ...| s_415.trec|   test|[things, were, go...|[things, good, wh...|[0.04348014305449...|\n",
      "| 353547_0_16|\\n\\nI know I shou...| s_354.trec|   test|[, , i, know, i, ...|[, , know, happy,...|[-0.0248732537791...|\n",
      "|s_1797_861_5|Its been very hum...|s_1797.trec|  train|[its, been, very,...|[humbling, watch,...|[0.01981742198404...|\n",
      "|s_2301_358_1|Cos I really want...|s_2301.trec|  train|[cos, i, really, ...|[cos, really, wan...|[-0.0339223429560...|\n",
      "|   25941_0_9|I thought because...|  s_26.trec|   test|[i, thought, beca...|[thought, vista, ...|[0.10666825083483...|\n",
      "| 311784_0_47|I told him I want...| s_312.trec|   test|[i, told, him, i,...|[told, wanted, le...|[0.03440016302435...|\n",
      "|  14826_0_39|She wouldn't acce...|  s_15.trec|   test|[she, wouldn't, a...|[accept, apology,...|[0.03320454593747...|\n",
      "|  410815_2_3|i feel like a tot...| s_411.trec|   test|[i, feel, like, a...|[feel, like, tota...|[8.50871499431760...|\n",
      "| 167162_0_26|He had the door o...| s_168.trec|   test|[he, had, the, do...|[door, van, open,...|[0.08273945665477...|\n",
      "|  546650_7_0|Okay here is the ...| s_547.trec|   test|[okay, here, is, ...|[okay, ugly, trut...|[0.10716180215822...|\n",
      "| 345409_1_22|So the whole week...| s_346.trec|   test|[so, the, whole, ...|[whole, week's, d...|[-0.0269908657413...|\n",
      "|   52011_8_1|I use scare quote...|  s_53.trec|   test|[i, use, scare, q...|[use, scare, quot...|[0.00407301577860...|\n",
      "|  450974_0_4|However, we've no...| s_451.trec|   test|[however,, we've,...|[however,, dating...|[0.08531044823272...|\n",
      "|s_1017_484_1|I suggest that yo...|s_1017.trec|  train|[i, suggest, that...|[suggest, look, s...|[-0.0533816423267...|\n",
      "|  203871_0_7|\\n**TL:DR** Playe...| s_204.trec|   test|[, **tl:dr**, pla...|[, **tl:dr**, pla...|[0.06753630738799...|\n",
      "|  261919_0_3|Sorry if it's kin...| s_262.trec|   test|[sorry, if, it's,...|[sorry, kinda, ha...|[0.07980207225773...|\n",
      "| 258058_0_39|\\n\\n**TL;DR** I'm...| s_259.trec|   test|[, , **tl;dr**, i...|[, , **tl;dr**, e...|[0.01778487433106...|\n",
      "| 377496_0_13|\\n\\nWe had no mon...| s_378.trec|   test|[, , we, had, no,...|[, , money, hand,...|[9.25600122755918...|\n",
      "| 343511_3_27|I've had to work ...| s_344.trec|   test|[i've, had, to, w...|[work, support, h...|[-0.0063481865774...|\n",
      "+------------+--------------------+-----------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19806893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark(\n",
    "    memory=\"30g\",\n",
    "    **{\n",
    "        \"spark.sql.parquet.enableVectorizedReader\": False,\n",
    "    },\n",
    ")\n",
    "df = spark.read.parquet(\"/mnt/data/erisk/task1/processed/data/word2vec/v3/data\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_759c626141cc"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run inference on the entire dataset\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.functions import array_to_vector, vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pipeline = PipelineModel.load(\n",
    "    f\"{local_prefix}/task1/processed/eval/logistic_word2vec/v3/model\"\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- docid: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- target_1_prediction: double (nullable = false)\n",
      " |-- target_10_prediction: double (nullable = false)\n",
      " |-- target_11_prediction: double (nullable = false)\n",
      " |-- target_12_prediction: double (nullable = false)\n",
      " |-- target_13_prediction: double (nullable = false)\n",
      " |-- target_14_prediction: double (nullable = false)\n",
      " |-- target_15_prediction: double (nullable = false)\n",
      " |-- target_16_prediction: double (nullable = false)\n",
      " |-- target_17_prediction: double (nullable = false)\n",
      " |-- target_18_prediction: double (nullable = false)\n",
      " |-- target_19_prediction: double (nullable = false)\n",
      " |-- target_2_prediction: double (nullable = false)\n",
      " |-- target_20_prediction: double (nullable = false)\n",
      " |-- target_21_prediction: double (nullable = false)\n",
      " |-- target_3_prediction: double (nullable = false)\n",
      " |-- target_4_prediction: double (nullable = false)\n",
      " |-- target_5_prediction: double (nullable = false)\n",
      " |-- target_6_prediction: double (nullable = false)\n",
      " |-- target_7_prediction: double (nullable = false)\n",
      " |-- target_8_prediction: double (nullable = false)\n",
      " |-- target_9_prediction: double (nullable = false)\n",
      " |-- target_1_probability: double (nullable = true)\n",
      " |-- target_10_probability: double (nullable = true)\n",
      " |-- target_11_probability: double (nullable = true)\n",
      " |-- target_12_probability: double (nullable = true)\n",
      " |-- target_13_probability: double (nullable = true)\n",
      " |-- target_14_probability: double (nullable = true)\n",
      " |-- target_15_probability: double (nullable = true)\n",
      " |-- target_16_probability: double (nullable = true)\n",
      " |-- target_17_probability: double (nullable = true)\n",
      " |-- target_18_probability: double (nullable = true)\n",
      " |-- target_19_probability: double (nullable = true)\n",
      " |-- target_2_probability: double (nullable = true)\n",
      " |-- target_20_probability: double (nullable = true)\n",
      " |-- target_21_probability: double (nullable = true)\n",
      " |-- target_3_probability: double (nullable = true)\n",
      " |-- target_4_probability: double (nullable = true)\n",
      " |-- target_5_probability: double (nullable = true)\n",
      " |-- target_6_probability: double (nullable = true)\n",
      " |-- target_7_probability: double (nullable = true)\n",
      " |-- target_8_probability: double (nullable = true)\n",
      " |-- target_9_probability: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 07:31:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " docid                 | 321176_0_16          \n",
      " dataset               | test                 \n",
      " target_1_prediction   | 0.0                  \n",
      " target_10_prediction  | 0.0                  \n",
      " target_11_prediction  | 0.0                  \n",
      " target_12_prediction  | 0.0                  \n",
      " target_13_prediction  | 0.0                  \n",
      " target_14_prediction  | 0.0                  \n",
      " target_15_prediction  | 0.0                  \n",
      " target_16_prediction  | 0.0                  \n",
      " target_17_prediction  | 0.0                  \n",
      " target_18_prediction  | 0.0                  \n",
      " target_19_prediction  | 0.0                  \n",
      " target_2_prediction   | 0.0                  \n",
      " target_20_prediction  | 0.0                  \n",
      " target_21_prediction  | 0.0                  \n",
      " target_3_prediction   | 0.0                  \n",
      " target_4_prediction   | 0.0                  \n",
      " target_5_prediction   | 0.0                  \n",
      " target_6_prediction   | 0.0                  \n",
      " target_7_prediction   | 0.0                  \n",
      " target_8_prediction   | 0.0                  \n",
      " target_9_prediction   | 0.0                  \n",
      " target_1_probability  | 2.133767929901608... \n",
      " target_10_probability | 2.060329336583954... \n",
      " target_11_probability | 3.672689076195112... \n",
      " target_12_probability | 3.747357979477783... \n",
      " target_13_probability | 0.0                  \n",
      " target_14_probability | 1.543210004228967... \n",
      " target_15_probability | 1.042508407795850... \n",
      " target_16_probability | 1.043171501563522... \n",
      " target_17_probability | 2.220446049250313... \n",
      " target_18_probability | 1.536883420527601... \n",
      " target_19_probability | 8.280295671347915E-9 \n",
      " target_2_probability  | 4.449267620998398... \n",
      " target_20_probability | 2.439007324994691E-7 \n",
      " target_21_probability | 0.0                  \n",
      " target_3_probability  | 6.365680819464536... \n",
      " target_4_probability  | 9.862035942442837E-6 \n",
      " target_5_probability  | 4.181648805001714E-8 \n",
      " target_6_probability  | 0.0                  \n",
      " target_7_probability  | 5.327473138305727E-6 \n",
      " target_8_probability  | 8.310975919689767E-7 \n",
      " target_9_probability  | 1.849255222718948... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 07:31:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vector_df = df.withColumn(\"word2vec\", array_to_vector(\"word2vec\"))\n",
    "\n",
    "transformed = pipeline.transform(vector_df)\n",
    "# transformed.printSchema()\n",
    "\n",
    "subset = transformed.select(\n",
    "    \"docid\",\n",
    "    \"dataset\",\n",
    "    # keep the predictions\n",
    "    *[c for c in transformed.columns if c.endswith(\"_prediction\")],\n",
    "    # let's also keep the probability of it being relevant\n",
    "    *[\n",
    "        vector_to_array(c).getItem(1).alias(c)\n",
    "        for c in transformed.columns\n",
    "        if c.endswith(\"_probability\")\n",
    "    ],\n",
    ")\n",
    "subset.printSchema()\n",
    "subset.show(vertical=True, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write the results to disk\n",
    "subset.repartition(32).write.parquet(\n",
    "    f\"{local_prefix}/task1/processed/subset/word2vec_logistic_predictions/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = spark.read.parquet(\n",
    "    f\"{local_prefix}/task1/processed/subset/word2vec_logistic_predictions/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|(relevance_count > 0)|   count|\n",
      "+---------------------+--------+\n",
      "|                 true|  877548|\n",
      "|                false|18929345|\n",
      "+---------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|relevance_count|   count|\n",
      "+---------------+--------+\n",
      "|            0.0|18929345|\n",
      "|            1.0|  474386|\n",
      "|            2.0|  163794|\n",
      "|            3.0|   51995|\n",
      "|            4.0|   86616|\n",
      "|            5.0|   41301|\n",
      "|            6.0|   35027|\n",
      "|            7.0|   14621|\n",
      "|            8.0|    6082|\n",
      "|            9.0|    1155|\n",
      "|           10.0|     296|\n",
      "|           11.0|    1239|\n",
      "|           12.0|     187|\n",
      "|           13.0|     808|\n",
      "|           14.0|       1|\n",
      "|           15.0|      30|\n",
      "|           16.0|       1|\n",
      "|           17.0|       2|\n",
      "|           19.0|       1|\n",
      "|           20.0|       2|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now let's figure out what percentage of the documtents are actually relevant to some degree\n",
    "from functools import reduce\n",
    "\n",
    "relevance = subset.withColumn(\n",
    "    \"relevance_count\",\n",
    "    # sum all the predictions and see if its greater than 0\n",
    "    reduce(\n",
    "        lambda a, b: a + b,\n",
    "        [subset[c] for c in subset.columns if c.endswith(\"_prediction\")],\n",
    "    ),\n",
    ")\n",
    "\n",
    "relevance.groupBy(F.expr(\"relevance_count > 0\")).count().show()\n",
    "relevance.groupBy(\"relevance_count\").count().orderBy(\"relevance_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- docid: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check how many overlap with the scores we computed before\n",
    "scored = spark.read.parquet(f\"{local_prefix}/task1/processed/data/count_scores/v1\")\n",
    "scored.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_scored = relevance.where(\"relevance_count > 0\").join(\n",
    "    scored.where(\"score > 6.5\"), \"docid\"\n",
    ")\n",
    "relevant_scored.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:====================================================>  (64 + 3) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|       docid|            score|                                                                                                                                                                                                    text|\n",
      "+------------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|s_1721_303_1|8.987321812850125|fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fu...|\n",
      "|s_1721_301_0|8.502891406705377|fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fu...|\n",
      "| s_694_152_0|7.784057002639929|He better Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik...|\n",
      "| s_694_188_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_161_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_143_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_155_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_162_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_166_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_167_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_153_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_159_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_157_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_174_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_146_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_149_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_175_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_185_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_148_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "| s_694_158_0|7.783640596221253|Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik Vik V...|\n",
      "+------------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.join(relevant_scored, \"docid\").select(\"docid\", \"score\", \"text\").orderBy(\n",
    "    F.desc(\"score\")\n",
    ").show(truncate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- query: integer (nullable = true)\n",
      " |-- q0: integer (nullable = true)\n",
      " |-- docid: string (nullable = true)\n",
      " |-- rel: integer (nullable = true)\n",
      "\n",
      "+-----+---+-------------+---+\n",
      "|query| q0|        docid|rel|\n",
      "+-----+---+-------------+---+\n",
      "|   21|  0| s_2061_384_1|  0|\n",
      "|    3|  0|  s_1551_51_6|  0|\n",
      "|    7|  0|s_1674_10_210|  0|\n",
      "|   16|  0| s_2606_919_3|  0|\n",
      "|    9|  0| s_614_1573_2|  0|\n",
      "|   15|  0| s_993_1060_1|  1|\n",
      "|    9|  0|  s_785_291_1|  0|\n",
      "|   19|  0| s_617_1608_3|  0|\n",
      "|   11|  0|  s_835_17_51|  0|\n",
      "|   13|  0|  s_3045_10_2|  0|\n",
      "|   14|  0| s_552_399_29|  0|\n",
      "|   15|  0|s_1404_332_10|  0|\n",
      "|   15|  0|s_1514_320_53|  0|\n",
      "|    5|  0| s_2606_507_5|  0|\n",
      "|   18|  0| s_2597_379_0|  0|\n",
      "|    6|  0|  s_363_13_29|  0|\n",
      "|    8|  0|  s_451_465_0|  0|\n",
      "|    7|  0| s_2986_379_1|  1|\n",
      "|   21|  0|s_3008_149_15|  0|\n",
      "|    1|  0| s_2724_252_1|  0|\n",
      "+-----+---+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# also read the relevant test documents\n",
    "train_labels_path = f\"{local_prefix}/task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_rels_consenso.csv\"\n",
    "labels = spark.read.csv(train_labels_path, header=True, inferSchema=True).repartition(\n",
    "    16\n",
    ")\n",
    "labels.printSchema()\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16148"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.select(\"docid\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 166:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|dataset|count|\n",
      "+-------+-----+\n",
      "|  train|16148|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.join(labels.select(\"docid\").distinct(), \"docid\").groupBy(\"dataset\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 09:14:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                ]]\r"
     ]
    }
   ],
   "source": [
    "# let's create a new dataset that only contains the most relevant documents for each class\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "primary_key = \"docid\"\n",
    "target_probs = [c for c in relevance.columns if c.endswith(\"_probability\")]\n",
    "top_docs = []\n",
    "filtered = (\n",
    "    relevance.where(\"relevance_count > 0\")\n",
    "    .where(\"dataset = 'test'\")\n",
    "    .join(scored.where(\"score < 6.5\").select(\"docid\"), \"docid\")\n",
    "    .cache()\n",
    ")\n",
    "for c in target_probs:\n",
    "    ordered = filtered.select(\n",
    "        F.lit(int(c.split(\"_\")[1])).alias(\"symptom_number\"),\n",
    "        primary_key,\n",
    "        F.col(c).alias(\"score\"),\n",
    "    ).where(F.col(\"score\") > 0.7)\n",
    "    top_docs.append(ordered)\n",
    "\n",
    "# union all the documents together\n",
    "k = 10_000\n",
    "docs = (\n",
    "    reduce(lambda a, b: a.union(b), top_docs)\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"symptom_number\").orderBy(F.desc(\"score\"))\n",
    "        ),\n",
    "    )\n",
    "    # keep the top 10_000 documents for each class\n",
    "    .where(F.col(\"rank\") <= k)\n",
    "    # now we only care about the documents\n",
    "    .select(primary_key)\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# let's write out the set of documents to disk\n",
    "docs.repartition(32).write.parquet(\n",
    "    f\"{local_prefix}/task1/processed/data/count_docids_relevant/v1\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59867"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = spark.read.parquet(\n",
    "    f\"{local_prefix}/task1/processed/data/count_docids_relevant/v1\"\n",
    ")\n",
    "docs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now that we have a much smaller set of documents to work with, we can embed these documents specifically and run inference on them\n",
    "(\n",
    "    df.join(docs, \"docid\")\n",
    "    .union(df.join(labels.select(\"docid\").distinct(), \"docid\"))\n",
    "    .select(\"docid\", \"dataset\", \"text\", \"word2vec\")\n",
    "    .repartition(64)\n",
    "    .write.parquet(\n",
    "        f\"{local_prefix}/task1/processed/data/word2vec_relevant/v1\", mode=\"overwrite\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 237:===========================>                          (16 + 16) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|dataset|count|\n",
      "+-------+-----+\n",
      "|  train|16148|\n",
      "|   test|59867|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res = spark.read.parquet(f\"{local_prefix}/task1/processed/data/word2vec_relevant/v1\")\n",
    "res.groupBy(\"dataset\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
