{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/30 19:23:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/30 19:23:48 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://erisk2-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x76d50c1284f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erisk.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=8, memory=\"28g\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   DOCNO|                POST|                 PRE|                TEXT|_corrupt_record|filename|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "|   0_0_6|I'm trying to wor...|\\n\\nAlthough most...|I will not tell m...|           NULL|s_0.trec|\n",
      "|456_1_12|You're not like '...|In general though...|Oh, and if you're...|           NULL|s_1.trec|\n",
      "| 764_1_5|Maybe it's one of...|My past experienc...|But I still want ...|           NULL|s_1.trec|\n",
      "|651_0_28|\\n\\nWe all run ba...|\\n\\nSo this woman...|I couldn't even i...|           NULL|s_1.trec|\n",
      "| 268_1_3| Both were great,...|\\n\\nI've only had...|One a couple year...|           NULL|s_1.trec|\n",
      "|364_0_12|I started opening...|Even though I too...|Which I can under...|           NULL|s_1.trec|\n",
      "|765_0_33|Nowhere on my inv...|I ask about the t...|Words can not exp...|           NULL|s_1.trec|\n",
      "|409_0_18|\\n\\nCOMMUNIST DAU...|The mention of a ...|The last stanza d...|           NULL|s_1.trec|\n",
      "| 546_1_1|~ Isabelle Stenge...|                    |On the contrary, ...|           NULL|s_1.trec|\n",
      "|582_0_11|Boys would shove ...|Everything went d...|They decided to t...|           NULL|s_1.trec|\n",
      "|785_0_15|He called things ...|\\n\\nWe were both ...|I ended up being ...|           NULL|s_1.trec|\n",
      "|657_0_30|\\n\\nTL;DR: I didn...|I'm not sure what...|I'm sorry this wa...|           NULL|s_1.trec|\n",
      "|253_1_31|\\n\\n**TL;DR: I wa...|Then I got nauseo...|\\n\\nAll in all, i...|           NULL|s_1.trec|\n",
      "| 661_0_4| I feel like utte...|\\n\\n\\nAbout a yea...|It turned out I w...|           NULL|s_1.trec|\n",
      "|  54_0_0|He wouldn't tell ...|                    |Over the past wee...|           NULL|s_1.trec|\n",
      "|670_0_27|She gets it's not...|\\n\\n\\nI think I e...|\\n\\n\\nSo in some ...|           NULL|s_1.trec|\n",
      "| 914_0_3|\\n\\n\\nHe's attemp...|He plans on visit...|He's studying to ...|           NULL|s_1.trec|\n",
      "|574_0_10|They can't send y...|\\n\\nYou are letti...|You're 20 and an ...|           NULL|s_1.trec|\n",
      "| 190_0_2|\\n\\nSo 7 or so mi...|\\n\\nA few nights ...|So i put her hand...|           NULL|s_1.trec|\n",
      "|678_0_29|All end on first ...|with**\\n\\n**EDIT*...|\\n\\n- Never had a...|           NULL|s_1.trec|\n",
      "+--------+--------------------+--------------------+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15542200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|   DOCNO|                TEXT|filename|\n",
      "+--------+--------------------+--------+\n",
      "| s_0_0_0|    1.ye katiliyorum|s_0.trec|\n",
      "| s_0_1_0|ok haklsn abi gol...|s_0.trec|\n",
      "| s_0_2_0| almanca yarrak gibi|s_0.trec|\n",
      "| s_0_3_0|hani u oyunlarn e...|s_0.trec|\n",
      "| s_0_3_1|dead cellste ygda...|s_0.trec|\n",
      "| s_0_3_2|bunlarn bir dili ...|s_0.trec|\n",
      "| s_0_4_0|lnce diriltiyor s...|s_0.trec|\n",
      "| s_0_6_0|       ziya gzel sal|s_0.trec|\n",
      "| s_0_7_0|  artk dedem deilsin|s_0.trec|\n",
      "| s_0_8_0|sorma bizim matem...|s_0.trec|\n",
      "| s_0_9_0|240 Volt FUCKMAST...|s_0.trec|\n",
      "|s_0_10_0|bunlar nerden evi...|s_0.trec|\n",
      "|s_0_11_0|beynine gidecek k...|s_0.trec|\n",
      "|s_0_12_0|semeyen vizyonsuz...|s_0.trec|\n",
      "|s_0_13_0|       ok haklsn abi|s_0.trec|\n",
      "|s_0_14_0|ilkokul zamanlari...|s_0.trec|\n",
      "|s_0_15_0|iliki kurmakta zo...|s_0.trec|\n",
      "|s_0_15_1|liseye giden bir ...|s_0.trec|\n",
      "|s_0_15_2|tipimin ve kiilii...|s_0.trec|\n",
      "|s_0_15_3|ben insanlarla ko...|s_0.trec|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4264693"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets also load up the train df\n",
    "train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "train_df.show()\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    Word2Vec,\n",
    "    StopWordsRemover\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# let's apply tf-idf to the text column, and also include word2vec\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# bucket = \"gs://dsgt-clef-erisk-2024\"\n",
    "# test_df = spark.read.parquet(f\"{bucket}/task1/parquet/test\")\n",
    "# train_df = spark.read.parquet(f\"{bucket}/task1/parquet/train\")\n",
    "\n",
    "# import sbert model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # create a pandas udf that will encode the text and return an array of doubles\n",
    "@F.pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "def encode(x: pd.Series) -> pd.Series:\n",
    "    return pd.Series(model.encode(x).tolist())\n",
    "\n",
    "\n",
    "word2vec_features = 64\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "            inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\"\n",
    "        )\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=word2vec_features,\n",
    "    numPartitions=8,\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol=\"word2vec\",\n",
    ")\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(stages=[tokenizer, remover, encode])\n",
    "\n",
    "# pipeline_model = pipeline.fit(train_df)\n",
    "# %time pipeline_model.transform(train_df.repartition(8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM TRANSFORMER ----------------------------------------------------------------\n",
    "class BertTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Wrapper for BERT to add it to the pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputCol=None, outputCol=None, append_str=None):\n",
    "        self.column = column\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "    # def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    #     df = df.drop(*[x for x in df.columns if any(y in x for y in self.banned_list)])\n",
    "    #     return df\n",
    "    \n",
    "    @F.pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "    def _transform(self, x: pd.Series) -> pd.Series:\n",
    "        return pd.Series(model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.pandas import Series\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Wrapper for BERT to add it to the pipeline\n",
    "\"\"\"\n",
    "# CUSTOM TRANSFORMER ----------------------------------------------------------------\n",
    "class BertTransformer(Transformer,               # Base class\n",
    "                     HasInputCol,               # Sets up an inputCol parameter\n",
    "                     HasOutputCol,              # Sets up an outputCol parameter\n",
    "                     DefaultParamsReadable,     # Makes parameters readable from file\n",
    "                     DefaultParamsWritable      # Makes parameters writable from file\n",
    "                    ):      \n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\")\n",
    "\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(BertTransformer, self).__init__()\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only     \n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "  \n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "            \n",
    "    def _transform(self, df: DataFrame):\n",
    "\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        \n",
    "        @F.pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "        def encode(x: pd.Series) -> pd.Series:\n",
    "            return pd.Series(model.encode(x).tolist())\n",
    "\n",
    "        \n",
    "        return df.withColumn(output_col, encode(input_col))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+--------------------+\n",
      "|         DOCNO|                TEXT|   filename|          embeddings|\n",
      "+--------------+--------------------+-----------+--------------------+\n",
      "|   s_1713_77_3|Obviously FedEx i...|s_1713.trec|[0.07817142456769...|\n",
      "| s_1147_156_12|The neurologist a...|s_1147.trec|[-0.0036929193884...|\n",
      "|    s_1401_9_6|On the enrollment...|s_1401.trec|[-0.0344385877251...|\n",
      "|  s_1899_304_3|Why is it so sens...|s_1899.trec|[0.13363982737064...|\n",
      "|   s_1473_88_1|A color code syst...|s_1473.trec|[-0.0758219882845...|\n",
      "|  s_1116_171_0| Jailbait wasn't ...|s_1116.trec|[-0.0268517620861...|\n",
      "|   s_1373_35_0|   u/repostsleuthbot|s_1373.trec|[-0.0888357460498...|\n",
      "|   s_2077_31_2|I'm guessing ther...|s_2077.trec|[0.04538367688655...|\n",
      "|  s_2137_639_0|Wegen Corona-Muta...|s_2137.trec|[-0.1068064272403...|\n",
      "|  s_1511_311_0|[Source](https://...|s_1511.trec|[-0.0483171567320...|\n",
      "| s_1360_403_48|They gave up the ...|s_1360.trec|[-0.0089962221682...|\n",
      "|s_2362_1280_33|\\tFrankenstein Un...|s_2362.trec|[-0.1123339831829...|\n",
      "| s_1367_266_61|The hazards of th...|s_1367.trec|[-0.0134083740413...|\n",
      "|   s_2157_87_5|If you and the TP...|s_2157.trec|[0.00796107295900...|\n",
      "|    s_18_264_0| *This is assumin...|  s_18.trec|[-0.0384435728192...|\n",
      "|  s_1226_311_0|    I like the shape|s_1226.trec|[-0.0525874197483...|\n",
      "|  s_1932_102_3|Sorry for any con...|s_1932.trec|[-0.0090472437441...|\n",
      "|  s_2488_839_1|Yea, I mean Miles...|s_2488.trec|[0.00509003177285...|\n",
      "|  s_1771_144_5|As for leeching m...|s_1771.trec|[-0.0447219461202...|\n",
      "| s_1515_1378_0|Families suffer a...|s_1515.trec|[-0.0474599227309...|\n",
      "+--------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 153 ms, sys: 197 ms, total: 350 ms\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer = BertTransformer(input_col=\"TEXT\", output_col=\"embeddings\")\n",
    "pipeline = Pipeline(stages=[transformer])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "%time pipeline_model.transform(train_df.repartition(8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_df = pipeline_model.transform(train_df)\n",
    "res_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "|  DOCNO|                                                                            TEXT|filename|                                                                      embeddings|\n",
      "+-------+--------------------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "|s_0_0_0|                                                                1.ye katiliyorum|s_0.trec|[0.0180697999894619, 0.004467027261853218, 0.020407328382134438, -4.309927753...|\n",
      "|s_0_1_0|                                                     ok haklsn abi gold atar msn|s_0.trec|[-0.13028191030025482, 0.0033819167874753475, -0.027139397338032722, 0.082881...|\n",
      "|s_0_2_0|                                                             almanca yarrak gibi|s_0.trec|[-0.02012443356215954, 0.08180119842290878, -0.08102694153785706, 0.043510053...|\n",
      "|s_0_3_0|hani u oyunlarn en gl en gizemli silah, zellii falan olan eylere garip isimle...|s_0.trec|[-0.04646919667720795, 0.08211159706115723, -0.021132497116923332, -0.0101455...|\n",
      "|s_0_3_1|dead cellste ygdar orus li ox var mesela, deus ex machina da olabilir onun gi...|s_0.trec|[-0.07952062040567398, 0.031551968306303024, -0.06744185090065002, 0.00103699...|\n",
      "+-------+--------------------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res_df.show(n=5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a new dataframe with features from both train and test\n",
    "\n",
    "total_df = (\n",
    "    train_df.select(\n",
    "        \"DOCNO\",\n",
    "        \"TEXT\",\n",
    "        \"filename\",\n",
    "        F.lit(\"train\").alias(\"dataset\"),\n",
    "    )\n",
    "    .union(\n",
    "        test_df.select(\n",
    "            \"DOCNO\",\n",
    "            F.concat(\n",
    "                F.coalesce(F.col(\"PRE\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"TEXT\"), F.lit(\"\")),\n",
    "                F.coalesce(F.col(\"POST\"), F.lit(\"\")),\n",
    "            ).alias(\"TEXT\"),\n",
    "            \"filename\",\n",
    "            F.lit(\"test\").alias(\"dataset\"),\n",
    "        ),\n",
    "    )\n",
    "    .where(\"filename is not null\")\n",
    "    .where(\"TEXT is not null\")\n",
    ").repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline model saved\n",
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- dataset: string (nullable = false)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                        (0 + 4) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47178)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o306.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m res_df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# save both the pipeline and the total_df\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mres_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/task1/parquet/combined_sbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined sbert saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o306.parquet"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(total_df)\n",
    "pipeline_model.write().overwrite().save(f\"{bucket}/task1/models/pipeline_sbert\")\n",
    "print(\"pipeline model saved\")\n",
    "\n",
    "res_df = pipeline_model.transform(total_df)\n",
    "res_df.printSchema()\n",
    "\n",
    "# save both the pipeline and the total_df\n",
    "res_df.write.mode(\"overwrite\").parquet(f\"{bucket}/task1/parquet/combined_sbert\")\n",
    "print(\"combined sbert saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
